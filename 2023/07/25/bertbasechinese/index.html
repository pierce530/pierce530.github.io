<!doctype html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8" >
    <meta http-equiv="X-UA-Compatible" content="IE=11,IE=10,IE=9,IE=8" >
    <meta name="baidu-site-verification" content="dIcXMeY8Ya" />
    
    <title>在Hugging Face上下载并微调Bert-base-Chinese | Pierce小屋</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0" >
    <meta name="keywords" content="Pierce, Web, 章靖宇, 开发" >
    <meta name="description" content="Pierce的个人博客" >

    
    <link rel="alternative" href="/atom.xml" title="Pierce小屋" type="application/atom+xml" >
    
    
    <link rel="icon" href="/img/favicon.ico" >
    
    
<link rel="stylesheet" href="/css/style.css?v=1732343092054.css">

    <!--[if lt IE 9]>
    
<script src="/js/html5.js"></script>

    <![endif]-->
    

<meta name="generator" content="Hexo 7.3.0"></head>

<body class="home">
    <!--[if lt IE 9]>
    <div class="browsehappy">
        当前网页 <strong>不支持</strong>
        你正在使用的浏览器. 为了正常的访问, 请 <a target="_blank" rel="noopener" href="http://browsehappy.com/">升级你的浏览器</a>.
    </div>
    <![endif]-->

    <!-- 博客头部 -->
    <header class="header">
    <section class="container header-main">
        <div class="logo">
            <a href="/">
                <div class="cover">
                    <span class="name">Pierce小屋</span>
                    <span class="description">欢迎访问</span>
                </div>
            </a>
        </div>
        <div class="dropnav iconfont icon-nav" id="JELON__btnDropNav"></div>
        <ul class="menu hidden" id="JELON__menu">
            
            <li rel="/2023/07/25/bertbasechinese/index.html" class="item ">
                <a href="/" title="首页" class="iconfont icon-home">&nbsp;首页</a>
            </li>
            
            <li rel="/2023/07/25/bertbasechinese/index.html" class="item ">
                <a href="/archives/" title="归档" class="iconfont icon-archives">&nbsp;归档</a>
            </li>
            
            <li rel="/2023/07/25/bertbasechinese/index.html" class="item ">
                <a href="/about/" title="关于" class="iconfont icon-staff">&nbsp;关于</a>
            </li>
            
            <li rel="/2023/07/25/bertbasechinese/index.html" class="item ">
                <a href="/comment/" title="留言" class="iconfont icon-comment">&nbsp;留言</a>
            </li>
            
        </ul>
        <div class="profile clearfix">
            <div class="feeds fl">
                
                
                <p class="links">
                    
                        <a href="https://github.com/pierce530" target="_blank">Github</a>
                        |
                    
                        <a href="https://blog.csdn.net/qq_43668800" target="_blank">CSDN</a>
                        
                    
                </p>
                <p class="sns">
                    
                        <a href="https://weibo.com/u/7756586759" class="sinaweibo" target="_blank"><b>■</b> 新浪微博</a>
                    
                        <a href="https://space.bilibili.com/351790592" class="bilibili" target="_blank"><b>■</b> bilibili</a>
                    
                    <a href="javascript: void(0);" class="wechat">
                        <b>■</b>
                        公众号
                        <span class="popover">
                            <img src="/img/wechat_mp.jpg" width="120" height="120" alt="我的微信订阅号">
                            <i class="arrow"></i>
                        </span>
                    </a>
                </p>
                
            </div>
            <div class="avatar fr">
                <img src="/img/avatar.jpg" alt="avatar" title="Pierce" >
            </div>
        </div>
    </section>
</header>


    <!-- 博客正文 -->
    <div class="container body clearfix">
        <section class="content">
            <div class="content-main widget">
                <!-- 文章页 -->
<!-- 文章 -->
<article class="post article">
    <header class="text-center">
        <h3 class="post-title"><span>在Hugging Face上下载并微调Bert-base-Chinese</span></h3>
    </header>
    <p class="post-meta text-center">
        Pierce 发表于
        <time datetime="2023-07-24T16:00:00.000Z">2023-07-25</time>
    </p>
    <div id="JELON__articlePostContent" class="post-content">
        <h2 id="Hugging-Face"><a href="#Hugging-Face" class="headerlink" title="Hugging Face"></a>Hugging Face</h2><p>Hugging face 起初是一家总部位于纽约的聊天机器人初创服务商，他们本来打算创业做聊天机器人，然后在github上开源了一个Transformers库，虽然聊天机器人业务没搞起来，但是他们的这个库在机器学习社区迅速大火起来。目前已经共享了超100,000个预训练模型，10,000个数据集，变成了机器学习界的github。</p>
<p>huggingface的官方网站：<a href="https://link.zhihu.com/?target=http://www.huggingface.co./">http://www.huggingface.co.</a> 在这里主要有以下大家需要的资源：</p>
<ol>
<li>Datasets：数据集，以及数据集的下载地址</li>
<li>Models：各个预训练模型</li>
<li>course：免费的nlp课程，可惜都是英文的</li>
<li>docs：文档</li>
</ol>
<h2 id="Bert-Base-Chinese"><a href="#Bert-Base-Chinese" class="headerlink" title="Bert_Base_Chinese"></a>Bert_Base_Chinese</h2><p>我们都知道BERT是一个强大的预训练模型，它可以基于谷歌发布的预训练参数在各个下游任务中进行微调。Bert_Base_Chinese已经针对中文进行了预训练，训练和随机输入掩蔽已独立应用于单词片段（如原始 BERT 论文中所示）。值得注意的是，Bert_Base_Chinese的单词token是一个一个中文汉字，而非词语。其他Bert相关内容非本文目的，不再赘述。</p>
<p><img src="/2023/07/25/bertbasechinese/01.png" alt="Bert的embedding原理"></p>
<h2 id="Bert-Tokenizer"><a href="#Bert-Tokenizer" class="headerlink" title="Bert_Tokenizer"></a>Bert_Tokenizer</h2><h3 id="简单编码器"><a href="#简单编码器" class="headerlink" title="简单编码器"></a>简单编码器</h3><p>Bert_Base_Chinese的下载地址：<a target="_blank" rel="noopener" href="https://huggingface.co/bert-base-chinese">https://huggingface.co/bert-base-chinese</a></p>
<p>自动下载模型与分词器方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 联网下载模型与分词器使用，地址存放在C:\Users\admin\.cache\huggingface\hub\models--bert-base-chinese</span></span><br><span class="line"></span><br><span class="line">model_name = <span class="string">&#x27;bert-base-chinese&#x27;</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">sents = [<span class="string">&quot;白日依山尽&quot;</span>, <span class="string">&quot;黄河入海流&quot;</span>]</span><br><span class="line">out = tokenizer.encode(</span><br><span class="line">    <span class="comment"># 传入的两个句子</span></span><br><span class="line">    text=sents[<span class="number">0</span>],</span><br><span class="line">    text_pair=sents[<span class="number">1</span>],</span><br><span class="line">    <span class="comment"># 长度大于设置是否截断</span></span><br><span class="line">    truncation=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># 一律补齐，如果长度不够</span></span><br><span class="line">    padding=<span class="string">&#x27;max_length&#x27;</span>,</span><br><span class="line">    add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">    max_length=<span class="number">30</span>,</span><br><span class="line">    return_tensors=<span class="literal">None</span>,</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(out))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>若网络不佳，该方法可能不适用，可使用前述网址手动下载，按下述方法手动导入，效果相同，其中手动导入时文件路径如图：</p>
<p><img src="/2023/07/25/bertbasechinese/02.png" alt="model文件路径"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里手动下载模型与分词器，根据目录加载使用</span></span><br><span class="line"></span><br><span class="line">vocab_file = <span class="string">&#x27;model/vocab.txt&#x27;</span></span><br><span class="line">tokenizer = BertTokenizer(vocab_file)</span><br><span class="line">bert = BertModel.from_pretrained(<span class="string">&quot;model/bert-base-chinese/&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里只是用到分词器，上一句调用模型不需要亦可</span></span><br><span class="line">sents = [<span class="string">&quot;白日依山尽&quot;</span>, <span class="string">&quot;黄河入海流&quot;</span>]</span><br><span class="line">out = tokenizer.encode(</span><br><span class="line">    <span class="comment"># 传入的两个句子</span></span><br><span class="line">    text=sents[<span class="number">0</span>],</span><br><span class="line">    text_pair=sents[<span class="number">1</span>],</span><br><span class="line">    <span class="comment"># 长度大于设置是否截断</span></span><br><span class="line">    truncation=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># 一律补齐，如果长度不够</span></span><br><span class="line">    padding=<span class="string">&#x27;max_length&#x27;</span>,</span><br><span class="line">    add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">    max_length=<span class="number">30</span>,</span><br><span class="line">    return_tensors=<span class="literal">None</span>,</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(out))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>两种方法下的运行结果如下图，可以看到由于人为设置每句应有30个字，故而在后面填充[PAD]，同时每一句的开头标识为[CLS]，中间分隔与结尾标识为[SEP]。如果使用len()，可知tokenizer长度共为21128。Bert接受一次输入一个句子或两个句子，此为常识，此不赘述。</p>
<p><img src="/2023/07/25/bertbasechinese/03.png" alt="诗句分词"></p>
<h3 id="增强编码器"><a href="#增强编码器" class="headerlink" title="增强编码器"></a>增强编码器</h3><p>有时使用简单编码器encode()不能满足需求，可以使用encode_plus()，相较前者该编码器可设置返回tensor类型，token标识类型，mask类型与特殊标识类型与返回长度等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里使用增强的编码器函数encode_plus</span></span><br><span class="line"></span><br><span class="line">vocab_file = <span class="string">&#x27;model/vocab.txt&#x27;</span></span><br><span class="line">tokenizer = BertTokenizer(vocab_file)</span><br><span class="line">bert = BertModel.from_pretrained(<span class="string">&quot;model/bert-base-chinese/&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里只是用到分词器，上一句调用模型不需要亦可</span></span><br><span class="line">sents = [<span class="string">&quot;白日依山尽&quot;</span>, <span class="string">&quot;黄河入海流&quot;</span>]</span><br><span class="line">out = tokenizer.encode_plus(</span><br><span class="line">    <span class="comment"># 传入的两个句子</span></span><br><span class="line">    text=sents[<span class="number">0</span>],</span><br><span class="line">    text_pair=sents[<span class="number">1</span>],</span><br><span class="line">    <span class="comment"># 长度大于设置是否截断</span></span><br><span class="line">    truncation=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># 一律补齐，如果长度不够</span></span><br><span class="line">    padding=<span class="string">&#x27;max_length&#x27;</span>,</span><br><span class="line">    add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">    max_length=<span class="number">30</span>,</span><br><span class="line">    <span class="comment"># 可取值tf,pt,np,（tensorflow,pytorch,numpy）默认返回list</span></span><br><span class="line">    return_tensors=<span class="literal">None</span>,</span><br><span class="line">    <span class="comment"># 返回token_type_ids,第一句与特殊符号是0，第二句是1</span></span><br><span class="line">    return_token_type_ids=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># 返回attention_mask，填充是0，其他是1</span></span><br><span class="line">    return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># 返回special_tokens_mask特殊符号标识，特殊是1，其他是0</span></span><br><span class="line">    return_special_tokens_mask=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># 返回长度</span></span><br><span class="line">    return_length=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> out.items():</span><br><span class="line">    <span class="built_in">print</span>(k, <span class="string">&#x27;:&#x27;</span>, v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(out[<span class="string">&#x27;input_ids&#x27;</span>]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以看到token_type_ids属性的返回值第一句全为0，第二句全为1，填充全为0；special_tokens_mask属性的返回值正常字符为0，开头，分隔，结尾，填充等特殊token全为1；attention_mask返回值关注有效部分，有效部分全为1，填充部分全为0；最后长度返回的是设置长度，即有效部分+填充部分总长度30。	</p>
<p><img src="/2023/07/25/bertbasechinese/04.png" alt="填充长度"></p>
<h3 id="批量编码器"><a href="#批量编码器" class="headerlink" title="批量编码器"></a>批量编码器</h3><p>更多情况下，并非一句一句将语句送入编码器，而是批量送入。故而最常用的编码器函数为batch_encode_plus()，实现如下，可以看到主要区别是句子输入的属性变为batch_text_or_text_pairs，是一个list属性，内部可以是一个句子也可以是一个tuple。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里使用批量的的编码器函数batch_encode_plus</span></span><br><span class="line"></span><br><span class="line">vocab_file = <span class="string">&#x27;model/vocab.txt&#x27;</span></span><br><span class="line">tokenizer = BertTokenizer(vocab_file)</span><br><span class="line">bert = BertModel.from_pretrained(<span class="string">&quot;model/bert-base-chinese/&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里只是用到分词器，上一句调用模型不需要亦可</span></span><br><span class="line">sents = [<span class="string">&quot;白日依山尽&quot;</span>, <span class="string">&quot;黄河入海流&quot;</span>, <span class="string">&quot;欲穷千里目&quot;</span>, <span class="string">&quot;更上一层楼&quot;</span>]</span><br><span class="line">out = tokenizer.batch_encode_plus(</span><br><span class="line">    <span class="comment"># 传入的所有句子，单句</span></span><br><span class="line">    <span class="comment"># batch_text_or_text_pairs=sents,</span></span><br><span class="line">    <span class="comment"># 传入的所有句子，有成对句子</span></span><br><span class="line">    batch_text_or_text_pairs=[sents[<span class="number">0</span>], (sents[<span class="number">1</span>], sents[<span class="number">2</span>]), sents[<span class="number">3</span>]],</span><br><span class="line">    <span class="comment"># 长度大于设置是否截断</span></span><br><span class="line">    truncation=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># 一律补齐，如果长度不够</span></span><br><span class="line">    padding=<span class="string">&#x27;max_length&#x27;</span>,</span><br><span class="line">    add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">    max_length=<span class="number">30</span>,</span><br><span class="line">    <span class="comment"># 可取值tf,pt,np,（tensorflow,pytorch,numpy）默认返回list</span></span><br><span class="line">    return_tensors=<span class="literal">None</span>,</span><br><span class="line">    <span class="comment"># 返回token_type_ids,第一句与特殊符号是0，第二句是1</span></span><br><span class="line">    return_token_type_ids=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># 返回attention_mask，填充是0，其他是1</span></span><br><span class="line">    return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># 返回special_tokens_mask特殊符号标识，特殊是1，其他是0</span></span><br><span class="line">    return_special_tokens_mask=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># 返回长度,这里的长度是真实长度，而非设置的长度30了</span></span><br><span class="line">    return_length=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> out.items():</span><br><span class="line">    <span class="built_in">print</span>(k, <span class="string">&#x27;:&#x27;</span>, v)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(out[<span class="string">&#x27;input_ids&#x27;</span>])):</span><br><span class="line">    <span class="built_in">print</span>(tokenizer.decode(out[<span class="string">&#x27;input_ids&#x27;</span>][i]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>运行结果如下，可以看到input_ids变为二维数组，同时length不再返回总长度，而是返回每个句子的实际长度。</p>
<p><img src="/2023/07/25/bertbasechinese/05.png" alt="实际长度"></p>
<h2 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h2><p>字典并非一成不变，可以自己在字典中新增token。新增token有add_tokens()与add_special_tokens()两个函数，分别用于新增字符token与新增特殊token。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里使用字典</span></span><br><span class="line"></span><br><span class="line">vocab_file = <span class="string">&#x27;model/vocab.txt&#x27;</span></span><br><span class="line">tokenizer = BertTokenizer(vocab_file)</span><br><span class="line">bert = BertModel.from_pretrained(<span class="string">&quot;model/bert-base-chinese/&quot;</span>)</span><br><span class="line"></span><br><span class="line">zidian = tokenizer.get_vocab()</span><br><span class="line"><span class="comment"># bert_base_chinese以字分词</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(zidian), <span class="string">&#x27; &#x27;</span>, <span class="built_in">len</span>(zidian), <span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;月光&#x27;</span> <span class="keyword">in</span> zidian, <span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;月&#x27;</span> <span class="keyword">in</span> zidian, <span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;光&#x27;</span> <span class="keyword">in</span> zidian)</span><br><span class="line"></span><br><span class="line">tokenizer.add_tokens(new_tokens=[<span class="string">&#x27;月光&#x27;</span>])</span><br><span class="line">tokenizer.add_special_tokens(&#123;<span class="string">&#x27;eos_token&#x27;</span>: <span class="string">&#x27;[EOS]&#x27;</span>&#125;)</span><br><span class="line">zidian = tokenizer.get_vocab()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;月光&#x27;</span> <span class="keyword">in</span> zidian, <span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;[EOS]&#x27;</span> <span class="keyword">in</span> zidian, <span class="string">&#x27; &#x27;</span>, <span class="built_in">len</span>(zidian))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>运行效果如下图，可见字典长度原先为21128，与之前tokenizer长度相同。并且由于Bert_Base_Chinese的分词是按字分词，所以”月光“不在字典中。当手动加入”月光“与特殊标识”[EOS]“后，字典的长度相应加2。</p>
<p><img src="/2023/07/25/bertbasechinese/06.png" alt="手动添词"></p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>本文使用数据集为ChnSentiCorp，且后续实验皆使用该数据集，数据集可在Hugging Face上下载，地址<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/seamew/ChnSentiCorp/tree/main%E3%80%82">https://huggingface.co/datasets/seamew/ChnSentiCorp/tree/main。</a></p>
<p>ChnSentiCorp是一个用于做情感分类的数据集，手动下载后文件路径如下：</p>
<p><img src="/2023/07/25/bertbasechinese/07.png" alt="数据集文件路径"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(path=<span class="string">&#x27;datasets&#x27;</span>, split=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(dataset))</span><br><span class="line"><span class="built_in">print</span>(dataset[<span class="number">0</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>验证数据集是否成功导入，查看数据集长度与第一条数据内容：</p>
<p><img src="/2023/07/25/bertbasechinese/08.png" alt="数据集第一条数据"></p>
<p><img src="/2023/07/25/bertbasechinese/09.png" alt="数据集总长度"></p>
<p>可以看到该数据集共有9600条数据，每条数据包括text与label，label部分是对text部分情感的判断。</p>
<h2 id="中文分类任务"><a href="#中文分类任务" class="headerlink" title="中文分类任务"></a>中文分类任务</h2><p>现在尝试使用之前所述内容开始第一个实验，中文分类。实验直接使用已经训练好的Bert_Base_Chinese，故而设置torch.no_grad()不进行梯度下降，而是在后面加一个全连接层，对16个句子进行二分类任务。为了节省时间，实验跑到300轮结束。</p>
<p>CPU版本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel, AdamW</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中文分类</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, split</span>):</span><br><span class="line">        <span class="variable language_">self</span>.dataset = load_dataset(path=<span class="string">&#x27;datasets&#x27;</span>, split=split)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>):</span><br><span class="line">        text = <span class="variable language_">self</span>.dataset[i][<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">        label = <span class="variable language_">self</span>.dataset[i][<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> text, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset = Dataset(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># len(dataset), dataset[0]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载字典和分词工具</span></span><br><span class="line">token = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">data</span>):</span><br><span class="line">    sents = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">    labels = [i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">    <span class="comment">#编码</span></span><br><span class="line">    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,</span><br><span class="line">                                   truncation=<span class="literal">True</span>,</span><br><span class="line">                                   padding=<span class="string">&#x27;max_length&#x27;</span>,</span><br><span class="line">                                   max_length=<span class="number">500</span>,</span><br><span class="line">                                   return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">                                   return_length=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">#input_ids:编码之后的数字</span></span><br><span class="line">    <span class="comment">#attention_mask:是补零的位置是0,其他位置是1</span></span><br><span class="line">    input_ids = data[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">    attention_mask = data[<span class="string">&#x27;attention_mask&#x27;</span>]</span><br><span class="line">    token_type_ids = data[<span class="string">&#x27;token_type_ids&#x27;</span>]</span><br><span class="line">    labels = torch.LongTensor(labels)</span><br><span class="line">    <span class="comment">#print(data[&#x27;length&#x27;], data[&#x27;length&#x27;].max())</span></span><br><span class="line">    <span class="keyword">return</span> input_ids, attention_mask, token_type_ids, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#数据加载器</span></span><br><span class="line">loader = torch.utils.data.DataLoader(dataset=dataset,</span><br><span class="line">                                     batch_size=<span class="number">16</span>,</span><br><span class="line">                                     collate_fn=collate_fn,</span><br><span class="line">                                     shuffle=<span class="literal">True</span>,</span><br><span class="line">                                     drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids,</span><br><span class="line">        labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(loader))</span><br><span class="line"><span class="built_in">print</span>(input_ids.shape, attention_mask.shape, token_type_ids.shape, labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载中文bert模型</span></span><br><span class="line"><span class="comment"># 加载预训练模型</span></span><br><span class="line">pretrained = BertModel.from_pretrained(<span class="string">&#x27;model/bert-base-chinese&#x27;</span>)</span><br><span class="line"><span class="comment"># 不训练,不需要计算梯度</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> pretrained.parameters():</span><br><span class="line">    param.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 模型试算</span></span><br><span class="line">out = pretrained(input_ids=input_ids,</span><br><span class="line">           attention_mask=attention_mask,</span><br><span class="line">           token_type_ids=token_type_ids)</span><br><span class="line"><span class="comment"># 16个句子，500个词每句，768维度每词</span></span><br><span class="line"><span class="built_in">print</span>(out.last_hidden_state.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义下游任务模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc = torch.nn.Linear(<span class="number">768</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, token_type_ids</span>):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            out = pretrained(input_ids=input_ids,</span><br><span class="line">                       attention_mask=attention_mask,</span><br><span class="line">                       token_type_ids=token_type_ids)</span><br><span class="line">        out = <span class="variable language_">self</span>.fc(out.last_hidden_state[:, <span class="number">0</span>])</span><br><span class="line">        out = out.softmax(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line"><span class="comment"># 输出16，2，意为16句话分为2分类</span></span><br><span class="line"><span class="comment"># print(model(input_ids=input_ids,</span></span><br><span class="line"><span class="comment">#      attention_mask=attention_mask,</span></span><br><span class="line"><span class="comment">#      token_type_ids=token_type_ids).shape)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">5e-4</span>)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids,</span><br><span class="line">        labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line">    out = model(input_ids=input_ids,</span><br><span class="line">                attention_mask=attention_mask,</span><br><span class="line">                token_type_ids=token_type_ids)</span><br><span class="line">    loss = criterion(out, labels)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        out = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line">        accuracy = (out == labels).<span class="built_in">sum</span>().item() / <span class="built_in">len</span>(labels)</span><br><span class="line">        <span class="built_in">print</span>(i, loss.item(), accuracy)</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">300</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    loader_test = torch.utils.data.DataLoader(dataset=Dataset(<span class="string">&#x27;validation&#x27;</span>),</span><br><span class="line">                                              batch_size=<span class="number">32</span>,</span><br><span class="line">                                              collate_fn=collate_fn,</span><br><span class="line">                                              shuffle=<span class="literal">True</span>,</span><br><span class="line">                                              drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids,</span><br><span class="line">            labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader_test):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">5</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="built_in">print</span>(i)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            out = model(input_ids=input_ids,</span><br><span class="line">                        attention_mask=attention_mask,</span><br><span class="line">                        token_type_ids=token_type_ids)</span><br><span class="line">        out = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line">        correct += (out == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">        total += <span class="built_in">len</span>(labels)</span><br><span class="line">    <span class="built_in">print</span>(correct / total)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>​		GPU版本，内容与CPU版本一致，效果相同，速度更快：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer, AdamW</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中文分类的cuda版本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 快速演示</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;device=&#x27;</span>, device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练模型</span></span><br><span class="line">pretrained = BertModel.from_pretrained(<span class="string">&#x27;model/bert-base-chinese/&#x27;</span>)</span><br><span class="line"><span class="comment"># 需要移动到cuda上</span></span><br><span class="line">pretrained.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不训练,不需要计算梯度</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> pretrained.parameters():</span><br><span class="line">    param.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义下游任务模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc = torch.nn.Linear(<span class="number">768</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, token_type_ids</span>):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            out = pretrained(input_ids=input_ids,</span><br><span class="line">                             attention_mask=attention_mask,</span><br><span class="line">                             token_type_ids=token_type_ids)</span><br><span class="line">        out = <span class="variable language_">self</span>.fc(out.last_hidden_state[:, <span class="number">0</span>])</span><br><span class="line">        out = out.softmax(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line"><span class="comment"># 同样要移动到cuda</span></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 后面的计算和中文分类完全一样，只是放在了cuda上计算</span></span><br><span class="line"><span class="comment"># 定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(torch.utils.data.Dataset):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, split</span>):</span><br><span class="line">        <span class="variable language_">self</span>.dataset = load_dataset(<span class="string">&#x27;datasets&#x27;</span>)[split]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>):</span><br><span class="line">        text = <span class="variable language_">self</span>.dataset[i][<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">        label = <span class="variable language_">self</span>.dataset[i][<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> text, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset = Dataset(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载字典和分词工具</span></span><br><span class="line">token = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">data</span>):</span><br><span class="line">    sents = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">    labels = [i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 编码</span></span><br><span class="line">    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,</span><br><span class="line">                                   truncation=<span class="literal">True</span>,</span><br><span class="line">                                   padding=<span class="string">&#x27;max_length&#x27;</span>,</span><br><span class="line">                                   max_length=<span class="number">500</span>,</span><br><span class="line">                                   return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">                                   return_length=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># input_ids:编码之后的数字</span></span><br><span class="line">    <span class="comment"># attention_mask:是补零的位置是0,其他位置是1</span></span><br><span class="line">    input_ids = data[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">    attention_mask = data[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">    token_type_ids = data[<span class="string">&#x27;token_type_ids&#x27;</span>].to(device)</span><br><span class="line">    labels = torch.LongTensor(labels).to(device)</span><br><span class="line">    <span class="comment">#print(data[&#x27;length&#x27;], data[&#x27;length&#x27;].max())</span></span><br><span class="line">    <span class="keyword">return</span> input_ids, attention_mask, token_type_ids, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载器</span></span><br><span class="line">loader = torch.utils.data.DataLoader(dataset=dataset,</span><br><span class="line">                                     batch_size=<span class="number">16</span>,</span><br><span class="line">                                     collate_fn=collate_fn,</span><br><span class="line">                                     shuffle=<span class="literal">True</span>,</span><br><span class="line">                                     drop_last=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids,</span><br><span class="line">        labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(len(loader))</span></span><br><span class="line"><span class="comment"># print(input_ids.shape, attention_mask.shape, token_type_ids.shape, labels)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">5e-4</span>)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids,</span><br><span class="line">        labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line">    out = model(input_ids=input_ids,</span><br><span class="line">                attention_mask=attention_mask,</span><br><span class="line">                token_type_ids=token_type_ids)</span><br><span class="line">    loss = criterion(out, labels)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        out = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line">        accuracy = (out == labels).<span class="built_in">sum</span>().item() / <span class="built_in">len</span>(labels)</span><br><span class="line">        <span class="built_in">print</span>(i, loss.item(), accuracy)</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">300</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    loader_test = torch.utils.data.DataLoader(dataset=Dataset(<span class="string">&#x27;validation&#x27;</span>),</span><br><span class="line">                                              batch_size=<span class="number">32</span>,</span><br><span class="line">                                              collate_fn=collate_fn,</span><br><span class="line">                                              shuffle=<span class="literal">True</span>,</span><br><span class="line">                                              drop_last=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids,</span><br><span class="line">            labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader_test):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">5</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="built_in">print</span>(i)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            out = model(input_ids=input_ids,</span><br><span class="line">                        attention_mask=attention_mask,</span><br><span class="line">                        token_type_ids=token_type_ids)</span><br><span class="line">        out = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line">        correct += (out == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">        total += <span class="built_in">len</span>(labels)</span><br><span class="line">    <span class="built_in">print</span>(correct / total)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>实验结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">0 0.7015336751937866 0.375</span><br><span class="line">5 0.6688764095306396 0.625</span><br><span class="line">10 0.6294441223144531 0.6875</span><br><span class="line">15 0.6134305596351624 0.8125</span><br><span class="line">20 0.5924522876739502 0.6875</span><br><span class="line">25 0.5516412258148193 0.9375</span><br><span class="line">30 0.5142663717269897 0.9375</span><br><span class="line">35 0.5576078295707703 0.75</span><br><span class="line">40 0.49155497550964355 0.9375</span><br><span class="line">45 0.5733523964881897 0.75</span><br><span class="line">50 0.47572264075279236 0.875</span><br><span class="line">55 0.4810750186443329 0.875</span><br><span class="line">60 0.584780216217041 0.6875</span><br><span class="line">65 0.4616132080554962 0.9375</span><br><span class="line">70 0.46279385685920715 0.9375</span><br><span class="line">75 0.5439828038215637 0.875</span><br><span class="line">80 0.5376919507980347 0.75</span><br><span class="line">85 0.44682416319847107 0.8125</span><br><span class="line">90 0.5576794743537903 0.75</span><br><span class="line">95 0.5322697162628174 0.75</span><br><span class="line">100 0.538276195526123 0.8125</span><br><span class="line">105 0.5001559853553772 0.875</span><br><span class="line">110 0.5196012258529663 0.8125</span><br><span class="line">115 0.4310966730117798 0.9375</span><br><span class="line">120 0.43399879336357117 0.9375</span><br><span class="line">125 0.5088990926742554 0.8125</span><br><span class="line">130 0.5933794975280762 0.6875</span><br><span class="line">135 0.5021435022354126 0.75</span><br><span class="line">140 0.5618072748184204 0.6875</span><br><span class="line">145 0.4775013327598572 0.875</span><br><span class="line">150 0.44216257333755493 0.875</span><br><span class="line">155 0.5286621451377869 0.75</span><br><span class="line">160 0.4359947443008423 0.875</span><br><span class="line">165 0.3895459473133087 1.0</span><br><span class="line">170 0.5000126957893372 0.8125</span><br><span class="line">175 0.3741750121116638 1.0</span><br><span class="line">180 0.4112277626991272 0.875</span><br><span class="line">185 0.4835755228996277 0.8125</span><br><span class="line">190 0.5347906351089478 0.8125</span><br><span class="line">195 0.47410687804222107 0.8125</span><br><span class="line">200 0.454181969165802 0.875</span><br><span class="line">205 0.5046591758728027 0.875</span><br><span class="line">210 0.37064653635025024 1.0</span><br><span class="line">215 0.4531233012676239 0.9375</span><br><span class="line">220 0.5533742308616638 0.75</span><br><span class="line">225 0.3597880005836487 1.0</span><br><span class="line">230 0.37045764923095703 1.0</span><br><span class="line">235 0.5207308530807495 0.75</span><br><span class="line">240 0.44153541326522827 0.875</span><br><span class="line">245 0.4212343990802765 0.9375</span><br><span class="line">250 0.4749773442745209 0.875</span><br><span class="line">255 0.3914490044116974 0.9375</span><br><span class="line">260 0.4207759201526642 0.9375</span><br><span class="line">265 0.4993809163570404 0.875</span><br><span class="line">270 0.41594651341438293 0.9375</span><br><span class="line">275 0.4802340865135193 0.875</span><br><span class="line">280 0.47708410024642944 0.8125</span><br><span class="line">285 0.4468970000743866 0.9375</span><br><span class="line">290 0.5039204359054565 0.8125</span><br><span class="line">295 0.41882529854774475 0.875</span><br><span class="line">300 0.500678539276123 0.8125</span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">0.91875</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以看到仅仅300轮后准确率已经到了百分之80左右，由于只加了一个全连接层训练速度很快，这个效率是惊人的。在验证集上验证时的准确率更是惊人的来到了百分之92左右。</p>
<h2 id="中文填空任务"><a href="#中文填空任务" class="headerlink" title="中文填空任务"></a>中文填空任务</h2><p>这是Bert常见的随机将token转为[mask]，进行类似完形填空的填空任务，预测出被掩盖掉的token内容。代码如下，内容与钱一个任务基本相同，区别是编码时认为将第15个token转变为[mask]，从而符合任务需求。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel, AdamW</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中文填空</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, split</span>):</span><br><span class="line">        dataset = load_dataset(path=<span class="string">&#x27;datasets&#x27;</span>, split=split)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">data</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(data[<span class="string">&#x27;text&#x27;</span>]) &gt; <span class="number">30</span></span><br><span class="line">        <span class="variable language_">self</span>.dataset = dataset.<span class="built_in">filter</span>(f)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.dataset)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>):</span><br><span class="line">        text = <span class="variable language_">self</span>.dataset[i][<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset = Dataset(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"><span class="comment"># len(dataset), dataset[0]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载字典和分词工具</span></span><br><span class="line">token = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="comment"># 编码</span></span><br><span class="line">    data = token.batch_encode_plus(batch_text_or_text_pairs=data,</span><br><span class="line">                                   truncation=<span class="literal">True</span>,</span><br><span class="line">                                   padding=<span class="string">&#x27;max_length&#x27;</span>,</span><br><span class="line">                                   max_length=<span class="number">30</span>,</span><br><span class="line">                                   return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">                                   return_length=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># input_ids:编码之后的数字</span></span><br><span class="line">    <span class="comment"># attention_mask:是补零的位置是0,其他位置是1</span></span><br><span class="line">    input_ids = data[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">    attention_mask = data[<span class="string">&#x27;attention_mask&#x27;</span>]</span><br><span class="line">    token_type_ids = data[<span class="string">&#x27;token_type_ids&#x27;</span>]</span><br><span class="line">    <span class="comment"># 把第15个词固定替换为mask</span></span><br><span class="line">    labels = input_ids[:, <span class="number">15</span>].reshape(-<span class="number">1</span>).clone()</span><br><span class="line">    input_ids[:, <span class="number">15</span>] = token.get_vocab()[token.mask_token]</span><br><span class="line">    <span class="comment"># print(data[&#x27;length&#x27;], data[&#x27;length&#x27;].max())</span></span><br><span class="line">    <span class="keyword">return</span> input_ids, attention_mask, token_type_ids, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载器</span></span><br><span class="line">loader = torch.utils.data.DataLoader(dataset=dataset,</span><br><span class="line">                                     batch_size=<span class="number">16</span>,</span><br><span class="line">                                     collate_fn=collate_fn,</span><br><span class="line">                                     shuffle=<span class="literal">True</span>,</span><br><span class="line">                                     drop_last=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids,</span><br><span class="line">        labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(len(loader))</span></span><br><span class="line"><span class="comment"># print(token.decode(input_ids[0]))</span></span><br><span class="line"><span class="comment"># print(token.decode(labels[0]))</span></span><br><span class="line"><span class="comment"># print(input_ids.shape, attention_mask.shape, token_type_ids.shape, labels.shape)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练模型</span></span><br><span class="line">pretrained = BertModel.from_pretrained(<span class="string">&#x27;model/bert-base-chinese&#x27;</span>)</span><br><span class="line"><span class="comment"># 不训练,不需要计算梯度</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> pretrained.parameters():</span><br><span class="line">    param.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 模型试算</span></span><br><span class="line"><span class="comment"># out = pretrained(input_ids=input_ids,</span></span><br><span class="line"><span class="comment">#           attention_mask=attention_mask,</span></span><br><span class="line"><span class="comment">#           token_type_ids=token_type_ids)</span></span><br><span class="line"><span class="comment"># print(out.last_hidden_state.shape)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义下游任务模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.decoder = torch.nn.Linear(<span class="number">768</span>, token.vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bias = torch.nn.Parameter(torch.zeros(token.vocab_size))</span><br><span class="line">        <span class="variable language_">self</span>.decoder.bias = <span class="variable language_">self</span>.bias</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, token_type_ids</span>):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            out = pretrained(input_ids=input_ids,</span><br><span class="line">                             attention_mask=attention_mask,</span><br><span class="line">                             token_type_ids=token_type_ids)</span><br><span class="line"></span><br><span class="line">        out = <span class="variable language_">self</span>.decoder(out.last_hidden_state[:, <span class="number">15</span>])</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line"><span class="comment"># print(model(input_ids=input_ids,</span></span><br><span class="line"><span class="comment">#       attention_mask=attention_mask,</span></span><br><span class="line"><span class="comment">#       token_type_ids=token_type_ids).shape)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">5e-4</span>)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids,</span><br><span class="line">            labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line">        out = model(input_ids=input_ids,</span><br><span class="line">                    attention_mask=attention_mask,</span><br><span class="line">                    token_type_ids=token_type_ids)</span><br><span class="line">        loss = criterion(out, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            out = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line">            accuracy = (out == labels).<span class="built_in">sum</span>().item() / <span class="built_in">len</span>(labels)</span><br><span class="line">            <span class="built_in">print</span>(epoch, i, loss.item(), accuracy)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    loader_test = torch.utils.data.DataLoader(dataset=Dataset(<span class="string">&#x27;test&#x27;</span>),</span><br><span class="line">                                              batch_size=<span class="number">32</span>,</span><br><span class="line">                                              collate_fn=collate_fn,</span><br><span class="line">                                              shuffle=<span class="literal">True</span>,</span><br><span class="line">                                              drop_last=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids,</span><br><span class="line">            labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader_test):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">15</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="built_in">print</span>(i)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            out = model(input_ids=input_ids,</span><br><span class="line">                        attention_mask=attention_mask,</span><br><span class="line">                        token_type_ids=token_type_ids)</span><br><span class="line">        out = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line">        correct += (out == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">        total += <span class="built_in">len</span>(labels)</span><br><span class="line">        <span class="built_in">print</span>(token.decode(input_ids[<span class="number">0</span>]))</span><br><span class="line">        <span class="built_in">print</span>(token.decode(labels[<span class="number">0</span>]), token.decode(labels[<span class="number">0</span>]))</span><br><span class="line">    <span class="built_in">print</span>(correct / total)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="中文句子相关性任务"><a href="#中文句子相关性任务" class="headerlink" title="中文句子相关性任务"></a>中文句子相关性任务</h2><p>这是另一个常见的Bert训练任务，对给出的两个句子，判断是否相关。代码如下，内容亦基本一致，改变不过sents属性改为一次送入两个句子，其他甚至全连接层都与第一个任务相同：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel, AdamW</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中文句子相关性</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, split</span>):</span><br><span class="line">        dataset = load_dataset(path=<span class="string">&#x27;datasets&#x27;</span>, split=split)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">data</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(data[<span class="string">&#x27;text&#x27;</span>]) &gt; <span class="number">40</span></span><br><span class="line">        <span class="variable language_">self</span>.dataset = dataset.<span class="built_in">filter</span>(f)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>):</span><br><span class="line">        text = <span class="variable language_">self</span>.dataset[i][<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">        <span class="comment"># 切分一句话为前半句和后半句</span></span><br><span class="line">        sentence1 = text[:<span class="number">20</span>]</span><br><span class="line">        sentence2 = text[<span class="number">20</span>:<span class="number">40</span>]</span><br><span class="line">        label = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 有一半的概率把后半句替换为一句无关的话</span></span><br><span class="line">        <span class="keyword">if</span> random.randint(<span class="number">0</span>, <span class="number">1</span>) == <span class="number">0</span>:</span><br><span class="line">            j = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(<span class="variable language_">self</span>.dataset) - <span class="number">1</span>)</span><br><span class="line">            sentence2 = <span class="variable language_">self</span>.dataset[j][<span class="string">&#x27;text&#x27;</span>][<span class="number">20</span>:<span class="number">40</span>]</span><br><span class="line">            label = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> sentence1, sentence2, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset = Dataset(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载字典和分词工具</span></span><br><span class="line">token = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">data</span>):</span><br><span class="line">    sents = [i[:<span class="number">2</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">    labels = [i[<span class="number">2</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 编码</span></span><br><span class="line">    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,</span><br><span class="line">                                   truncation=<span class="literal">True</span>,</span><br><span class="line">                                   padding=<span class="string">&#x27;max_length&#x27;</span>,</span><br><span class="line">                                   max_length=<span class="number">45</span>,</span><br><span class="line">                                   return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">                                   return_length=<span class="literal">True</span>,</span><br><span class="line">                                   add_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># input_ids:编码之后的数字</span></span><br><span class="line">    <span class="comment"># attention_mask:是补零的位置是0,其他位置是1</span></span><br><span class="line">    <span class="comment"># token_type_ids:第一个句子和特殊符号的位置是0,第二个句子的位置是1</span></span><br><span class="line">    input_ids = data[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">    attention_mask = data[<span class="string">&#x27;attention_mask&#x27;</span>]</span><br><span class="line">    token_type_ids = data[<span class="string">&#x27;token_type_ids&#x27;</span>]</span><br><span class="line">    labels = torch.LongTensor(labels)</span><br><span class="line">    <span class="comment"># print(data[&#x27;length&#x27;], data[&#x27;length&#x27;].max())</span></span><br><span class="line">    <span class="keyword">return</span> input_ids, attention_mask, token_type_ids, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载器</span></span><br><span class="line">loader = torch.utils.data.DataLoader(dataset=dataset,</span><br><span class="line">                                     batch_size=<span class="number">8</span>,</span><br><span class="line">                                     collate_fn=collate_fn,</span><br><span class="line">                                     shuffle=<span class="literal">True</span>,</span><br><span class="line">                                     drop_last=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids,</span><br><span class="line">        labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(len(loader))</span></span><br><span class="line"><span class="comment"># print(token.decode(input_ids[0]))</span></span><br><span class="line"><span class="comment"># print(input_ids.shape, attention_mask.shape, token_type_ids.shape, labels)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练模型</span></span><br><span class="line">pretrained = BertModel.from_pretrained(<span class="string">&#x27;model/bert-base-chinese&#x27;</span>)</span><br><span class="line"><span class="comment"># 不训练,不需要计算梯度</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> pretrained.parameters():</span><br><span class="line">    param.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 模型试算</span></span><br><span class="line">out = pretrained(input_ids=input_ids,</span><br><span class="line">           attention_mask=attention_mask,</span><br><span class="line">           token_type_ids=token_type_ids)</span><br><span class="line"><span class="comment"># print(out.last_hidden_state.shape)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义下游任务模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc = torch.nn.Linear(<span class="number">768</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, token_type_ids</span>):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            out = pretrained(input_ids=input_ids,</span><br><span class="line">                             attention_mask=attention_mask,</span><br><span class="line">                             token_type_ids=token_type_ids)</span><br><span class="line"></span><br><span class="line">        out = <span class="variable language_">self</span>.fc(out.last_hidden_state[:, <span class="number">0</span>])</span><br><span class="line">        out = out.softmax(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line"><span class="comment"># print(model(input_ids=input_ids,</span></span><br><span class="line"><span class="comment">#       attention_mask=attention_mask,</span></span><br><span class="line"><span class="comment">#       token_type_ids=token_type_ids).shape)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">5e-4</span>)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids,</span><br><span class="line">        labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line">    out = model(input_ids=input_ids,</span><br><span class="line">                attention_mask=attention_mask,</span><br><span class="line">                token_type_ids=token_type_ids)</span><br><span class="line">    loss = criterion(out, labels)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        out = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line">        accuracy = (out == labels).<span class="built_in">sum</span>().item() / <span class="built_in">len</span>(labels)</span><br><span class="line">        <span class="built_in">print</span>(i, loss.item(), accuracy)</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">300</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    loader_test = torch.utils.data.DataLoader(dataset=Dataset(<span class="string">&#x27;test&#x27;</span>),</span><br><span class="line">                                              batch_size=<span class="number">32</span>,</span><br><span class="line">                                              collate_fn=collate_fn,</span><br><span class="line">                                              shuffle=<span class="literal">True</span>,</span><br><span class="line">                                              drop_last=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> i, (input_ids, attention_mask, token_type_ids,</span><br><span class="line">            labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader_test):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">5</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="built_in">print</span>(i)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            out = model(input_ids=input_ids,</span><br><span class="line">                        attention_mask=attention_mask,</span><br><span class="line">                        token_type_ids=token_type_ids)</span><br><span class="line">        pred = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line">        correct += (pred == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">        total += <span class="built_in">len</span>(labels)</span><br><span class="line">    <span class="built_in">print</span>(correct / total)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从上述三个例子，可以看出Bert预训练的巨大优势，再面对不同下游任务时，完全不需重新从头训练模型，BERT只需要微调就可以适应很多类型的NLP任务，这使其应用场景扩大，显著降低了训练与使用成本。</p>

    </div>
    <p class="eof">-- EOF --</p>
    <p class="post-meta">
        <span class="post-cat">分类：
            <a class="cat-link" href="/categories/Python/">Python</a>
        </span>
        <span class="post-tags">
            标签：
            
    
        <a href="/tags/Python/" title="Python">Python</a> / 
    
        <a href="/tags/AI/" title="AI">AI</a> / 
    
        <a href="/tags/Bert/" title="Bert">Bert</a>
    

        </span>
    </p>
</article>
<!-- 分享按钮 -->

  <div class="article-share clearfix text-center">
    <div class="share-area">
      <span class="share-txt">分享到：</span>
      <a href="javascript: window.open('http://service.weibo.com/share/share.php?url=' + encodeURIComponent(location.href) + '&title=' + document.title + '&language=zh_cn');" class="share-icon weibo"></a>
      <a href="javascript: alert('请复制链接到微信并发送');" class="share-icon wechat"></a>
      <a href="javascript: window.open('http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=' + encodeURIComponent(location.href) + '&title=' + document.title);" class="share-icon qqzone"></a>
      <a href="javascript: window.open('http://connect.qq.com/widget/shareqq/index.html?url=' + encodeURIComponent(location.href) + '&desc=Jelon个人博客&title=' + document.title + '&callback=' + encodeURIComponent(location.href));" class="share-icon qq"></a>
      <a href="javascript: window.open('http://shuo.douban.com/!service/share?href=' + encodeURIComponent(location.href) + '&name=' + document.title + '&text=' + document.title);" class="share-icon douban"></a>
    </div>
  </div>


<!-- 上一篇/下一篇 -->

<div class="article-nav clearfix">
    
    <span class="prev fl">
        上一篇<br >
        <a href="/2024/06/30/langchain-chatrobot/">
            
                LangChain 实现聊天机器人基础
            
        </a>
    </span>
    

    
    <span class="next fr">
        下一篇<br >
        <a href="javascript: void(0);">没有下一篇了</a>
    </span>
    
</div>

<!-- 文章评论 -->

  
<script src="/js/comment.js?v=1732343092044.js"></script>

  <div id="comments" class="comment">
    <!--
    <div class="sign-bar">
      GitHub 已登录!
      <span class="sign-link">登出</span>
    </div>
    <section class="box">
      <div class="com-avatar"><img src="/img/jelon.jpg" alt="avatar"></div>
      <div class="com-text">
        <div class="main">
          <textarea class="text-area-edited show" placeholder="欢迎评论！"></textarea>
          <div class="text-area-preview"></div>
        </div>
        <div class="switch">
          <div class="switch-item on">编辑</div>
          <div class="switch-item">预览</div>
        </div>
        <div class="button">提交</div>
      </div>
    </section>
    <section class="tips">注：评论支持 markdown 语法！</section>
    <section class="list-wrap">
      <ul class="list">
        <li>
          <div class="user-avatar">
            <a href="/">
              <img src="/img/jelon.jpg" alt="user-avatar">
            </a>
          </div>
          <div class="user-comment">
            <div class="user-comment-header">
              <span class="post-name">张德龙</span>
              <span class="post-time">2017年12月12日</span>
              <span class="like liked">已赞</span>
              <span class="like-num">2</span>
            </div>
            <div class="user-comment-body">333333</div>
          </div>
        </li>
        <li>
          <div class="user-avatar">
            <a href="/">
              <img src="/img/jelon.jpg" alt="user-avatar">
            </a>
          </div>
          <div class="user-comment">
            <div class="user-comment-header">
              <span class="post-name">刘德华</span>
              <span class="post-time">2017年12月12日</span>
              <span class="like">点赞</span>
              <span class="like-num">2</span>
            </div>
            <div class="user-comment-body">vvvvv</div>
          </div>
        </li>
      </ul>
      <div class="page-nav">
        <a href="javascript: void(0);" class="item">1</a>
        <a href="javascript: void(0);" class="item">2</a>
        <a href="javascript: void(0);" class="item current">3</a>
      </div>
    </section>
    -->
  </div>
  <script>
  JELON.Comment({
    container: 'comments',
    label: 'bertbasechinese' || '2023/07/25/bertbasechinese/',
    owner: '',
    repo: '',
    clientId: '',
    clientSecret: ''
  });
  </script>



<p class="text-center">
    <img style="width: 258px; height: 258px; border: none; padding: 0; border-radius: 3px;" src="/img/wechat_reward.jpg">
</p>
<p class="text-center">
    如果觉得博客对您有用，欢迎微信赞赏
</p>

            </div>
        </section>
        <!-- 侧栏部分 -->
<aside class="sidebar">
    
    <section class="widget">
        <h3 class="widget-hd"><strong>文章搜索</strong></h3>
        <div class="search-form">
  <form
    id="searchForm"
    method="GET"
    action="https://www.baidu.com/s"
    ectype="application/x-www-form-urlencoded"
    target="_blank"
    autocomplete="false"
    onsubmit="javascript: return false;">
    <input
      id="searchKeyword"
      type="text"
      class="form-control"
      placeholder="输入关键字搜索"
      autocomplete="false"
    />
    <input id="searchKeywordHidden" type="hidden" name="wd" />
    <input id="searchButton" class="btn" type="submit" value="搜索" />
  </form>
</div>
    </section>
    

    <section class="widget">
        <h3 class="widget-hd"><strong>文章分类</strong></h3>
        <!-- 文章分类 -->
<ul class="widget-bd">
    
    <li>
        <a href="/categories/Python/">Python</a>
        <span class="badge">(3)</span>
    </li>
    
    <li>
        <a href="/categories/JavaScript/">JavaScript</a>
        <span class="badge">(1)</span>
    </li>
    
</ul>
    </section>

    
    <section class="widget">
        <h3 class="widget-hd"><strong>热门标签</strong></h3>
        <!-- 文章标签 -->
<div class="widget-bd tag-wrap">
  
    <a class="tag-item" href="/tags/Python/" title="Python">Python (3)</a>
  
    <a class="tag-item" href="/tags/AI/" title="AI">AI (3)</a>
  
    <a class="tag-item" href="/tags/LangChain/" title="LangChain">LangChain (2)</a>
  
    <a class="tag-item" href="/tags/Agent/" title="Agent">Agent (2)</a>
  
    <a class="tag-item" href="/tags/Bert/" title="Bert">Bert (1)</a>
  
    <a class="tag-item" href="/tags/WEB%E6%8A%80%E6%9C%AF/" title="WEB技术">WEB技术 (1)</a>
  
    <a class="tag-item" href="/tags/%E5%89%8D%E7%AB%AF/" title="前端">前端 (1)</a>
  
    <a class="tag-item" href="/tags/JavaScript/" title="JavaScript">JavaScript (1)</a>
  
</div>
    </section>
    

    

    
</aside>
<!-- / 侧栏部分 -->
    </div>

    <!-- 博客底部 -->
    <footer class="footer">
    &copy;
    
        2014-2024
    

    <a href="/">Pierce Love You Three Thousand</a>
</footer>
<div class="back-to-top" id="JELON__backToTop" title="返回顶部">返回顶部</div>

    <!--博客js脚本 -->
    <!-- 这里放网站js脚本 -->

<script src="/js/main.js?v=1732343092054.js"></script>


</body>
</html>
